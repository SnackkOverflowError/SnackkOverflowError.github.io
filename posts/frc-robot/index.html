<!DOCTYPE html>
<html lang="en-us">
    <script src="https://cdn.jsdelivr.net/gh/tameemsafi/typewriterjs/dist/core.js"></script><head>
    <title>Frc Robot</title>
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="description" content="" />
    <style>
@import "https://fonts.googleapis.com/css2?family=Inconsolata&display=swap";#content{position:absolute;top:50%;left:50%;width:800px;height:600px;min-width:250px;max-width:800px;margin-left:-400px;margin-top:-300px;font-family:inconsolata,monospace;font-size:24px;line-height:28px;font-weight:400}









body{background:#282828}body #terminal{color:#ebdbb2}body #user{color:#98971a}body #dir{color:#458588}body .Typewriter__cursor{color:#ebdbb2}a{color:#ebdbb2}</style>

</head><body><div id="content">
    
    

    
    
    <div id=typewriter></div>

<script type="text/javascript">
    const instance = new Typewriter('#typewriter', {
        strings: '\u003cspan id=terminal\u003e\u003ch1 id=\u0027title\u0027\u003eFrc Robot\u003c\/h1\u003e\u003ch3 id=\u0022tldr\u0022\u003etldr\u003c\/h3\u003e\n\u003cp\u003eDeveloped code for 3 125 lb robots that competes in FRC competitions. The 2020 robot uses PIDF and computer vision to accurately shoot balls into an 8 foot high target. It was able to pickup and automatically indexballs, and rotate a colour wheel a specific number of times using a colour sensor.\u003c\/p\u003e\n\u003ch3 id=\u00222020\u0022\u003e2020\u003c\/h3\u003e\n\u003cp\u003eIn 2020, I was the programming advisor and helped to design and implement the software for the robot. In 2020, the robot had to intake small balls and shoot them at a target 8 feet in the air. It also had to rotate a color wheel and hang off of a tilted bar. Our robot in 2020 had a turret and shooter system, a drivetrain, an intake. I played a part in the development and testing of each of these systems and also played a part in their integration.\nFor the turret and shooter system we used a vision camera called LimeLight to identify the target 8 feet up. Using PID we were able to rotate the turret horizontal to face the target. Then. using the size of the target from the vision camera we were able to build a relation between the size of the target in the camera and the distance to the target. We were also able to find a relationship between the distance to the target and the required speed and vertical angle required to hit the target. Using these two systems we were able to build an automatic targeting system that could auto adjust as the robot was moving and accurately hit their target.\nFor the color wheel mechanism we used a colour sensor connected to the robot using I2C to detect the number of rotations the color had moved, as well as the current color that the wheel was on.\u003c\/p\u003e\n\u003ch3 id=\u00222019\u0022\u003e2019\u003c\/h3\u003e\n\u003cp\u003eIn 2019 I was the programming lead and organized and ran the design subteam. I lead the overall design of the software and aided with the implementations of the software. I was responsible for the testing as well as the fixes to the software (sometimes requiring complete redesigns of the subsystems). The robot in 2019 had to pick up discs and large balls and put them in tall towers.\nTo accomplish this task, the robot had 4 main components that had to work together: the drivetrain, the vision camera, the 2 stage cascade elevator, and then claw. Due to poor visibility on the field, a lot of the actions the robot had to accomplish such as picking up discs from the “loading station” and placing them on their appropriate target, or putting balls in the appropriate target had to be automated using computer vision. I implemented a “driver assist” that allowed the robot to drive into the target, using the vision target. It used a PID feedback controller to keep the robot angled at the target as the driver drove in. Further on in the season, I decided to completely automate those tasks. I logged and compared the input from the vision target with the distance from the target, to build a relationship between the two. That allowed me to develop software that automatically drove the robot to the target, complete an action (such as scoring a disc) and and then drive away. This completely removed driver involvement in game actions and allowed the robot to score points faster than the drivers could, with very few errors.\nThe elevator was made using the built in motion profiling on our motor controllers as well as velocity PID. We found the necessary setpoints required of the elevator and allowed the drivers to join to those setpoints. We also gave the drivers manual control of the elevator, using position PID. This is because the elevator was extremely fast, so running it without some kind of feedback control was risky, as it could damage the system. We developed quite a few failsafes like that into the robot, so that the drivers didn’t need to worry about damaging the robot or operating it incorrectly. For example we noticed that the robot started to jerk around and begin tipping over if the elevator was at max height. So we added in software to limit the max speed of the robot based on the height of the elevator. We also added open loop ramps on the drive train to reduce jerking around or tipping over when the drivers stopped abruptly.\u003c\/p\u003e\n\u003ch3 id=\u00222018\u0022\u003e2018\u003c\/h3\u003e\n\u003cp\u003eIn 2018, I was a member of the software subteam on my robotics team. I was responsible for developing and maintaining software for a single subsystem. The game that year required the robot to pick up cubes from the ground and drop them into a tall scale as well as two small scales.\nThe subsystem I was responsible for was the 2 stage cascading elevator. I implemented a state machine that managed the elevators change in state as it was going from one set point to another. I used PID to accurately get the elevator to go to its targets.\u003c\/p\u003e\n\u003c\/span\u003e',
        delay: '1',
        autoStart: true,
    });
</script>

        </div></body>
</html>
